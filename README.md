# Bias_Score_MBBQ
This repository contains a short code to calculate the bias score with the outputs of the MBBQ benchmark used in the paper ['Cross-linguistic Social Bias in LLMs: Investigating the
Role of Grammatical Gender and Hofstedeâ€™s Masculinity vs. Femininity Dimension.'](https://theses.liacs.nl/3548)

# Paper abstract
Large Language Models (LLMs) have become increasingly influential due to their widespread use across various applications across many countries. While LLMs have rapidly revolutionized various applications of Natural Language Processing (NLP), they also exhibit biases that are embedded within them. These biases can be harmful, as they may raise ethical concerns and reinforce social inequalities. Notably, such biases are less present in English than in other languages. To address these disparities, multilingual large language models MLLMs have been developed. However, recent research shows that despite their multilingual training, MLLMs often reproduce Western-centric cultural norms and values, and attempts to obtain culturally aligned responses via language prompting have had limited success. This highlights the ongoing challenges in accurately representing diverse cultural erspectives across languages. This thesis investigates social bias in LLMs across multiple languages, with a focus on the relationship between linguistic gender categories and cultural masculinity. The study uses the Language Index of grammatical gender, the cultural model of Hofstede, and the MBBQ benchmark to examine these factors. Our findings suggest that social bias in LLMs varies across languages, but do demonstrate consistent patterns. The variations and consistent patterns can be attributed by grammatical gender, but cannot be attributed solely to the degree of cultural masculinity. Instead, the relationship between language, gender categories, cultural asculinity, and bias is more nuanced. The relationship depends on the context of the question and the bias category to which it belongs. These results highlight the need for more nuanced and intersectional approaches to understanding and mitigating bias in MLLMs.
